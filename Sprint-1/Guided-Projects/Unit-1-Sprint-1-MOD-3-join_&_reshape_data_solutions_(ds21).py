# -*- coding: utf-8 -*-
"""Join & Reshape Data Solutions (DS21).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lryc4eyhG0pfosBRtk0yN-4cmXAMHPM6

# Join & Reshape Data

## Objectives

- Concatenate data using the pandas concat method
- Merge data using pandas merge
- Define the concept of tidy data and describe the format
- Transition between tidy and wide data formats with the `melt` and `pivot` methods

![The Anatomy of a DataFrame and a Series](https://miro.medium.com/max/700/1*ZSehcrMtBWN7_qCWq_HiSg.png)

## Concatenating Dataframes with Pandas

"Concatenate" is a fancy word for joining two things together. For example, we can concatenate two strings together using the `+` operator.
"""

'We can join/concatenate two strings together ' + 'using the "+" operator.'

"""When we "concatenate" two dataframes we will "stick them together" either by rows or columns. Lets look at some simple examples:"""

import pandas as pd

df1 = pd.DataFrame({'a': [1,2,3,4], 'b': [4,5,6,7], 'c': [7,8,9,10]})

df2 = pd.DataFrame({'a': [6,4,8,7], 'b': [9,4,3,2], 'c': [1,6,2,9]})

df1.head()

df2.head()

"""### Concatenate by Rows 

concatenating by rows is the default behavior of `pd.concat()` This is often the most common form of concatenation. 
"""

by_rows = pd.concat([df1, df2])

by_rows

by_rows.reset_index()

"""### Concatenate by Columns"""

by_cols = pd.concat([df1, df2], axis=1)

by_cols

by_cols['a']

# rename columns

by_cols.columns = ['a1', 'b1', 'c1', 'a2', 'b2', 'c2']

by_cols

by_cols['a1']

"""When concatenating dataframes, it is done using the column headers and row index values to match rows up. If these don't match up, then `NaN` values will be added where matches can't be found. """

df3 = pd.DataFrame({'a': [4,3,2,1], 'b': [4,5,6,7], 'c': [7,8,9,10]})

df4 = pd.DataFrame({'a': [6,4,8,7,8], 'b': [9,4,3,2,1], 'd': [1,6,2,9,5]})

df3.head()

df4.head()

"""### Concatenate by rows when not all column headers match"""

by_rows = pd.concat([df3, df4])

by_rows

"""### Concatenate by columns when not all row indexes match"""

by_cols = pd.concat([df3, df4], axis=1)

by_cols

"""Whenever we are combining dataframes, if appropriate values cannot be found based on the rules of the method we are using, then missing values will be filled with `NaNs`.

### Let's Try It Out

We’ll work with a dataset of [3 Million Instacart Orders, Open Sourced](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2)!

The files that we will be working with are in a folder of CSVs, we need to load that folder of CSVs, explore the CSVs to make sure that we understand what we're working with, and where the important data lies, and then work to combine the dataframes together as necessary. 



Our goal is to reproduce this table which holds the first two orders for user id 1.
"""

from IPython.display import display, Image
url = 'https://cdn-images-1.medium.com/max/1600/1*vYGFQCafJtGBBX5mbl0xyw.png'
example = Image(url=url, width=600)

display(example)

!wget https://www.dropbox.com/s/pofcl26lvoj6073/instacart-market-basket-analysis.zip

!unzip instacart-market-basket-analysis.zip

# Commented out IPython magic to ensure Python compatibility.
# %cd instacart-market-basket-analysis

# Unzip all .csv.zip files in the directory
!unzip "*.zip"

# List all csv files in the current directory
# -l specifies the "long" listing format, which includes additional info on each file
# -h specifies "human readable" file size units
!ls -l -h *.csv

display(example)

"""#### aisles

We don't need anything from aisles.csv
"""

aisles = pd.read_csv('aisles.csv')

print(aisles.shape)
aisles.head()

"""#### departments

We don't need anything from departments.csv
"""

departments = pd.read_csv('departments.csv')

print(departments.shape)
departments.head()

"""#### order_products__prior

We need:
- order id
- product id
- add to cart order

Everything except for 'reordered'
"""

order_products__prior = pd.read_csv('order_products__prior.csv')

print(order_products__prior.shape)
order_products__prior.head()

"""#### order_products__train

We need:
- order id
- proudct id
- add to cart order

Everything except for 'reordered'

Do you see anything similar between order_products__train and order_products__prior?


"""

order_products__train = pd.read_csv('order_products__train.csv')

print(order_products__train.shape)
order_products__train.head()

"""#### orders

We need:
- order id
- user id
- order number
- order dow
- order hour of day
"""

orders = pd.read_csv('orders.csv')

print(orders.shape)
orders.head()

"""#### products

We need:
- product id
- product name
"""

products = pd.read_csv('products.csv')

print(products.shape)
products.head()

"""#### Concatenate order_products__prior and order_products__train"""

order_products = pd.concat([order_products__prior, order_products__train])

order_products.head()

print(order_products__prior.shape)
print(order_products__train.shape)

print(order_products.shape)

"""### Key Point(s)

Concatenating dataframes means to stick two dataframes together either by rows or by columns. The default behavior of `pd.concat()` is to take the rows of one dataframe and add them to the rows of another dataframe. If we pass the argument `axis=1` then we will be adding the columns of one dataframe to the columns of another dataframe.

Concatenating dataframes is most useful when the columns are the same between two dataframes or when we have matching row indices between two dataframes. 

Be ready to use this method to combine dataframes together during your assignment.

## Merging Dataframes with Pandas
"""

display(example)

"""Before we can continue we need to understand where the data in the above table is coming from and what why specific pieces of data are held in the specific dataframes.

Each of these CSVs has a specific unit of observation (row). The columns that we see included in each CSV were selected purposefully. For example, everything each row of the `orders` dataframe is a specific and unique order -telling us who made the order, and when they made it. Every row in the `products` dataframe tells us about a specific and unique product that thestore offers. And everything in the `order_products` dataframe tells us about how products are associated with specific orders -including when the product was added to the shopping cart. 

### The Orders Dataframe

Holds information about specific orders, things like who placed the order, what 

- user_id
- order_id
- order_number
- order_dow
- order_hour_of_day

### The Products Dataframe

Holds information about individual products.

- product_id
- product_name

### The Order_Products Dataframe

Tells us how products are associated with specific orders since an order is a group of products.

- order_id
- product_id
- add_to_cart_order

As we look at the table that we're trying to recreate, we notice that we're not looking at specific orders or products, but at a specific **USER**. We're looking at the first two orders for a specific user and the products associated with those orders, so we'll need to combine dataframes to get all of this data together into a single table.

**The key to combining all of this information is that we need values that exist in both datasets that we can use to match up rows and combine dataframes.**

### Let's Try It Out
We have two dataframes, so we're going to need to merge our data twice. As we approach merging datasets together we will take the following approach.

1) Identify which to dataframes we would like to combine.

2) Find columns that are common between both dataframes that we can use to match up information.

3) Slim down both of our dataframes so that they only relevant data before we merge.

4) Merge the dataframes.
"""

merge_one = pd.merge(order_products, orders, how='inner', on='order_id')

merge_one.head()

all_info = pd.merge(merge_one, products, how='inner', on='product_id')

all_info.head()

"""#### ^^^^^ DON'T DO THIS!

I just merged absolutely everything


"""

## all_info2 = pd.merge(merge_one, )

"""#### First Merge

1) Combine `orders` and `order_products`

2) We will use the `order_id` column to match information between the two datasets

3) Lets slim down our dataframes to only the information that we need. We do this because the merge process is complex. Why would we merge millions of rows together if we know that we're only going to need 11 rows when we're done

What specific conditions could we use to slim down the `orders` dataframe?

`user_id == 1` and `order_id <=2`

or

`order_id == 2539329` and `order_id == 2398795`
"""

# An example of dataframe filtering

# Create a condition
condition = orders['order_id'] <=5

# Pass that condition into the square brackets 
# that we use to access portions of a dataframe
# only the rows where that condition evaluates to *TRUE*
# will be retained in the dataframe

# Look at the subsetted dataframe
orders[condition]

display(example)

# We don't necessarily have to save our condition to the variable "condition"
# we can pass the condition into the square bracket directly
# I just wanted to be clear what was happening inside of the square brackets

orders[(orders['order_id'] <=5)]

# orders[0:10,0:5]

orders['user_id'] == 1

orders['order_number'] <= 2

# Filter based on user_id and order_number
# AND condition version 
# I need to use the "bitwise" and operator: &

condition = (orders['user_id'] == 1) & (orders['order_number'] <= 5)

orders_subset = orders[condition]

orders_subset

"""Remember there are multiple ways that we could have filtered this dataframe. We also could have done it by specific `order_id`s

"""

# use the bitwise "or" operator: |

condition = (orders['order_id'] == 2539329) | (orders['order_id'] == 2398795)

orders_subset = orders[condition]

orders_subset

"""Now we'll filter down the order_products dataframe

What conditions could we use for subsetting that table?

We can use order_id again.
"""

condition = (order_products['order_id'] == 2539329) | (order_products['order_id'] == 2398795)

order_products_subset = order_products[condition]

order_products_subset

"""4) Now we're ready to merge these two tables together."""

orders_and_products = pd.merge(orders_subset, order_products_subset, on='order_id')

orders_and_products

display(example)

# Remove columns that we don't need

uneeded_cols = ['eval_set', 'days_since_prior_order', 'add_to_cart_order', 'reordered']

# orders_and_products.drop(uneeded_cols, inplace=True, axis=1)

orders_and_products

"""Okay, we're looking pretty good, we're missing one more column `product_name` so we're going to need to merge one more time

1) merge `orders_and_products` with `products`

2) Use `product_id` as our identifier in both tables

3) We need to slim down the `products` dataframe
"""

condition = products['product_id'].isin(orders_and_products['product_id'])

products_subset = products[condition]

products_subset



final = pd.merge(products_subset, orders_and_products, on='product_id')

final

final = final.drop(['aisle_id', 'department_id'], axis=1)

final

display(example)

"""#### Some nitpicky cleanup:"""

# sort rows
final = final.sort_values(by=['order_id'])

final

# reorder columns
final = final[['user_id', 'order_id', 'order_number','order_dow', 'order_hour_of_day', 'product_id', 'product_name']]

final

# remove underscores from column headers

final.columns = [column.replace('_', ' ') for column in final]

final

display(example)

"""### Challenge

Review this Chis Albon documentation about [concatenating dataframes by row and by column](https://chrisalbon.com/python/data_wrangling/pandas_join_merge_dataframe/) and then be ready to master this function and practice using different `how` parameters on your assignment.

## Tidy Data Format

### What is the Tidy Data Format?

In 2014, Hadley Wickham published an awesome paper named Tidy Data, that describes the process of tidying a dataset in R.

#### Conditions of Tidy Data

1. Each variable must have its own column.
2. Each observation must have its own row.

![Tidy Data Diagrams](https://miro.medium.com/max/700/1*7jjzhy4KknPz9hJVnC_w7w.png)

> A helpful mindset for determining whether your data are tidy is to think backwards from the plot you want to draw. From this perspective, a “variable” is something that will be assigned a role in the plot."

### Wide Data vs Long Data

- Long data = Separates the unit of analysis (country-year) into two separate variables.

![Long Data](https://tk-assets.lambdaschool.com/59e0b466-1a11-4561-b4e5-2ccbe800941b_long-data.png)

- Wide data = Combines one of the keys (year) with the value variable (avgtemp).
![Wide Data](https://tk-assets.lambdaschool.com/a168bf92-0696-494e-8366-ca18e7fb6742_wide-data.png)


#### The Case For Long Data
- More variables (columns) with wide data = **harder to summarise at a glance**
-  Structuring data as key-value pairs — as is done in long-form datasets — facilitates **conceptual clarity**. For example, in `country_long` above, it is clear that the unit of analysis is country-year — or, put differently, that the variables `country` and `year` jointly constitute the key in the dataset.
- Long-form datasets are often required for **advanced statistical analysis** and graphing. Furthermore, many graphing packages, rely on your data being in long form.


#### Wide Data
- Wide data format may be required for plotting (e.g. box plots)

### Why reshape data?

- Some libraries prefer data in different formats

For example, the Seaborn data visualization library prefers data in "Tidy" format often (but not always).

> "[Seaborn will be most powerful when your datasets have a particular organization.](https://seaborn.pydata.org/introduction.html#organizing-datasets) This format ia alternately called “long-form” or “tidy” data and is described in detail by Hadley Wickham. The rules can be simply stated:

### Hadley Wickham's Examples

From his paper, [Tidy Data](http://vita.had.co.nz/papers/tidy-data.html)
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import numpy as np
import seaborn as sns

table1 = pd.DataFrame(
    [[np.nan, 2],
     [16,    11], 
     [3,      1]],
    index=['John Smith', 'Jane Doe', 'Mary Johnson'], 
    columns=['treatmenta', 'treatmentb'])

""""Table 1 provides some data about an imaginary experiment in a format commonly seen in the wild. 

The table has two columns and three rows, and both rows and columns are labelled."
"""

table1

""""There are many ways to structure the same underlying data. 

Table 2 shows the same data as Table 1, but the rows and columns have been transposed. The data is the same, but the layout is different."
"""

# When we swap rows and columns we call that "transposing"
table2 = table1.T

table2

""""Table 3 reorganises Table 1 to make the values, variables and obserations more clear.

Table 3 is the tidy version of Table 1. Each row represents an observation, the result of one treatment on one person, and each column is a variable."

| name         | trt | result |
|--------------|-----|--------|
| John Smith   | a   | -      |
| Jane Doe     | a   | 16     |
| Mary Johnson | a   | 3      |
| John Smith   | b   | 2      |
| Jane Doe     | b   | 11     |
| Mary Johnson | b   | 1      |

### Let's Try It Out

#### Table 1 --> Tidy

We can use the pandas `melt` function to reshape Table 1 into Tidy format.
"""

table1

# Take the row index, and add it as a new column
table1 = table1.reset_index()

table1

# What is the unique identifier for each row
# Where is the data at that I want to be in my single "tidy" column
tidy = table1.melt(id_vars='index', value_vars=['treatmenta', 'treatmentb'])

tidy

# rename columns
tidy1 = tidy.rename(columns={ 'index': 'name', 'variable': 'trt', 'value': 'result'})

tidy1

tidy1.trt = tidy1.trt.str.replace('treatment', '')

tidy1

"""#### Tidy --> Table 1

The `pivot_table` function is the inverse of `melt`.
"""

# index: unique identifier
# columns: What do you want to differentiate the columns in wide format
# values: Where are the numbers at - go in the middle of the wide dataframe

wide = tidy1.pivot_table(index='name', columns='trt', values='result')

wide

"""### Challenge

On your assignment, be prepared to take table2 (the transpose of table1) and reshape it to be in tidy data format using `.melt()` and then put it back in "wide format" using `.pivot_table()`

## Transition between tidy and wide data formats with `.melt()` and `.pivot()`.

Tidy data format can be particularly useful with certain plotting libraries like Seaborn for example. Lets practice reshaping our data and show how this can be extremely useful in preparing our data for plotting.

Remember that tidy data format means:

- Each variable is a column
- Each observation is a row

A helpful mindset for determining whether your data are tidy is to think backwards from the plot you want to draw. From this perspective, a “variable” is something that will be assigned a role in the plot." When plotting, this typically means that the values that we're most interested in and that represent the same thing will all be in a single column. You'll see that in the different examples that we show. The important data will be in a single column.
"""

# Look at some of the awesome out-of-the-box seaborn functionality:

import seaborn as sns

sns.catplot(x='trt', y='result', col='name', kind='bar', data=tidy1, height=4);

"""### Let's Try It Out

Now with Instacart Data. We're going to try and reproduce a small part of this visualization: 
"""

from IPython.display import display, Image
url = 'https://cdn-images-1.medium.com/max/1600/1*wKfV6OV-_1Ipwrl7AjjSuw.png'
example = Image(url=url, width=600)

display(example)

"""Instead of a plot with 50 products, we'll just do two — the first products from each list
- Half And Half Ultra Pasteurized
- Half Baked Frozen Yogurt

So, given a `product_name` we need to calculate its `order_hour_of_day` pattern.
"""

products = pd.read_csv('products.csv')

order_products = pd.concat([pd.read_csv('order_products__prior.csv'), 
                            pd.read_csv('order_products__train.csv')])

orders = pd.read_csv('orders.csv')

"""#### Subset and Merge

One challenge of performing a merge on this data is that the `products` and `orders` datasets do not have any common columns that we can merge on. Due to this we will have to use the `order_products` dataset to provide the columns that we will use to perform the merge.

Here's the two products that we want to work with.
"""

product_names = ['Half Baked Frozen Yogurt', 'Half And Half Ultra Pasteurized']

"""Lets remind ourselves of what columns we have to work with:"""

orders.head()

orders.columns.to_list()

order_products.columns.to_list()

display(example)

"""This might blow your mind, but we're going to subset the dataframes to select specific columns **and** merge them all in one go. Ready?"""

merged = (products[['product_id', 'product_name']]
          .merge(order_products[['order_id', 'product_id']])
          .merge(orders[['order_id', 'order_hour_of_day']]))


print(merged.shape)
merged.head()

"""Ok, so we were a little bit lazy and probably should have subsetted our the rows of our dataframes before we merged them. We are going to filter after the fact. This is something that you can try out for practice. Can you figure out how to filter these dataframes **before** merging rather than after?"""

product_names = ['Half Baked Frozen Yogurt', 'Half And Half Ultra Pasteurized']

condition = merged['product_name'].isin(product_names) 

subset = merged[condition]

print(subset.shape)
subset.head()

"""Again, there are multiple effective ways to write conditions. """

condition = ((merged['product_name']=='Half Baked Frozen Yogurt') | 
             (merged['product_name']=='Half And Half Ultra Pasteurized'))

subset = merged[condition]

print(subset.shape)
subset.head()

subset['product_name'].value_counts()

"""#### 4 ways to reshape and plot


"""

display(example)

"""1) The `.value_counts()` approach.

Remember, that we're trying to get the key variables (values) listed as a single column.
"""

froyo = subset[subset['product_name']=='Half Baked Frozen Yogurt']
cream = subset[subset['product_name']=='Half And Half Ultra Pasteurized']

froyo.head()

cream.head()

cream['order_hour_of_day'].value_counts(normalize=True).sort_index()

froyo['order_hour_of_day'].value_counts(normalize=True).sort_index()

import matplotlib.pyplot as plt

(cream['order_hour_of_day'].value_counts(normalize=True).sort_index()
 .plot())
(froyo['order_hour_of_day'].dvalue_counts(normalize=True).sort_index()
 .plot());

display(example)

"""2) Crosstab"""

wide = pd.crosstab(subset['order_hour_of_day'], 
            subset['product_name'], 
            normalize='columns')

wide

wide.plot();

"""3) Pivot Table"""

subset.pivot_table(index='order_hour_of_day', columns='product_name', values='order_id', aggfunc=len)

subset.pivot_table(index='order_hour_of_day', columns='product_name', values='order_id', aggfunc=len).plot();

"""4) Melt 

We've got to get it into wide format first. We'll use a crosstab which is a specific type of pivot_table.
"""

melted = wide.reset_index().melt(id_vars='order_hour_of_day')

melted

melted = melted.rename(columns={
     'order_hour_of_day': 'Hour of Day Ordered', 
     'product_name': 'Product', 
     'value': 'Percent of Orders by Product'
 })

melted

"""Now, with Seaborn:"""

sns.relplot(x='Hour of Day Ordered', 
            y='Percent of Orders by Product', 
            hue='Product',
            data=melted,
            kind='line');

