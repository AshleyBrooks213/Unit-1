# -*- coding: utf-8 -*-
"""Make Features (DS21).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mdVQAa3KJjpx8iN_IRzoMfgJ4d6CxvJf

# Make Features

## Objectives

- Understand **the purpose of feature engineering**
- **Work with strings** in Pandas
- **Modify or create columns** of a dataframe using the `.apply()` function
- Work with **dates and times** in Pandas

## Feature Engineering

> #### Feature Engineering is the process of using a combination of domain knowledge, creativity and the pre-existing columns of a dataset to create completely new columns.

Machine Learning models try to detect patterns in the data and then associate those patterns with certain predictions. The hope is that by creating new columns on our dataset that we can expose our model to new patterns in the data so that it can make better and better predictions.

This is largely a matter of understanding how to work with individual columns of a dataframe with Pandas, which is what we'll be practicing today!

#### Feature Construction

- New features can be created from existing features by combining, splitting apart, or otherwise modifying existing features. 

- An example using data that consists of a calendar date would be to split the date variable into years, months, and day. A new feature could then be the year or the month by itself.

#### The Process

- Brainstorming or testing features
- Deciding what features to create
- Creating features
- Checking how the features work with your model
- Improving your features if needed
- Go back to brainstorming/creating more features until the work is complete

### Getting Started

Columns of a dataframe hold each hold a specific type of data. Lets inspect some of the common datatypes found in datasets and then we'll make a new feature on a dataset using pre-existing columns.
"""

import pandas as pd

# Pandas Display Options:
pd.set_option('display.max_rows', 150)
pd.set_option('display.max_columns', 100)

# Lets take a look at the Ames Iowa Housing Dataset:
df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/Ames%20Housing%20Data/train.csv')

!curl https://raw.githubusercontent.com/ryanleeallred/datasets/master/Ames%20Housing%20Data/train.csv

print(df.shape)
df.head()

"""### Analyzing Column Data Types"""

# Let's view the column types
df.dtypes

"""Some columns hold integer values like the `BedroomAbvGr` which stands for "Bedrooms Above Grade." This is the number of non-basement bedrooms in the home.

For more information on specific column meanings view the [data dictionary](https://github.com/ryanleeallred/datasets/blob/master/Ames%20Housing%20Data/data_description.txt).
"""

# Look at the first ten rows of the `BedroomAbvGr` column.
# Looks like integers to me!
df['BedroomAbvGr']

"""Some columns hold float values like the `LotFrontage` column."""

# Look at the first ten rows of the `LotFrontage` column.
df['LotFrontage']

"""Hmmm, do the values above look like floats to you?

They all have .0 on them so technically they're being stored as floats, but *should* they be stored as floats?

Lets see what all of the possible values for this column are.
"""

df['LotFrontage'].value_counts()

"""Looks to me like the `LotFrontage` column originally held integer values but was cast to a `float` meaning that each original integer values was converted to its corresponding float representation. 

Any guesses as to why that would have happened?


HINT: What's the most common `LotFrontage` value for this column?
"""

# NaN is the most common value in this column. What is a NaN
df['LotFrontage'].value_counts(dropna=False)

df.isnull().sum()

"""`NaN` stands stands for "Not a Number" and is the default missing value indicator with Pandas. This means there were cells in this column that didn't have a LotFrontage value recorded for those homes. 

This is where domain knowledge starts to come in. Think about the context we're working with here: houses. What might a null or blank cell representing "Linear feet of street connected to property" mean in the context of a housing dataset?

Ok, so maybe it makes seanse to have some NaNs in this column. What is the datatype of a NaN value?

Perhaps some of this data is truly missing or unrecorded data, but sometimes `NaNs` are more likely to indicate something that was "NA" or "Not Applicable" to a particular observation. There could be multiple reasons why there was no value recorded for a particular feature.

Remember, that Pandas tries to maintain a single datatype for all values in a column, and therefore...
"""

import numpy as np

# What is the datatype of NaN?
type(np.NaN)

"""The datatype of a NaN is float!  This means that if we have a column of integer values, but the column has even a single `NaN` that column will not be treated with the integer datatype but all of the integers will be converted to floats in order to try and preserve the same datatype throughout the entire column.

You can see already how understanding column datatypes is crucial to understanding how Pandas help us manage our data.
"""

df['BedroomAbvGr']

# dot syntax does not work in every circumstance
df.BedroomAbvGr

df['BedroomAbvGr'][0] = np.NaN

df['BedroomAbvGr']

"""### Making New Features

Lets slim down the dataset and consider just a few specific columns:

- `TotalBsmtSF`
- `1stFlrSF`
- `2ndFlrSF`
- `SalePrice`

"""

# get a single column
df['TotalBsmtSF']

# Get more than one column, then pass in a list of column headers

df[['TotalBsmtSF', '1stFlrSF']]

# I can make a smaller dataframe with a few specific column headers
# by passing a list of column headers inside of the square brackets
small_df = df[['TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'SalePrice']].copy()

small_df.head()

small_df.dtypes

"""### Creating New Columns

When making a new column on a dataframe, we have to use the square bracket syntax of accessing a column. We can't use "dot syntax" here.
"""

# Lets add up all of the square footage to get a single square footage 
# column for the entire dataset

# Using bracket syntax to make a new 'TotalSquareFootage' column
small_df['TotalSquareFootage'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']

small_df.head()

# Lets make a nother new column that is 'PricePerSqFt' by
# dividing the price by the square footage
small_df['PricePerSquareFoot'] = small_df['SalePrice'] / small_df['TotalSquareFootage']

small_df.head()
# small_df.dtypes

"""Okay, we have made two new columns on our small dataset.

- What does a **high** `PricePerSqFt` say about a home that the square footage and price alone don't capture as directly?

- What does a **low** `PricePerSqFt` say about a home that the square footage and price alone don't directly capture?

## Working with Strings in Pandas

So far we have worked with numeric datatypes (ints and floats) but we haven't worked with any columns containing string values. We can't simply use arithmetic to manipulate string values, so we'll need to learn some more techniques in order to work with this datatype.

We're going to import a new dataset here to work with. This dataset is from LendingClub and holds information about loans issued in Q4 of 2018. This dataset is a bit messy so it will give us plenty of opportunities to clean up existing columns as well as create new ones.

The `!wget` shell command being used here does exactly the same thing that your browser does when you type a URL in the address. It makes a request or "gets" the file at that address. However, in our case the file isn't a webpage, it's a compressed CSV file. 

Try copying and pasting the URL from below into your browser, did it start an automatic download? Any URLs like this that start automatic downloads when navigated to can be used along with the `!wget` command to bring files directly into your notebook's memory.

### Let's Load a New Dataset
"""

!wget https://resources.lendingclub.com/LoanStats_2018Q4.csv.zip

"""We need to use the `!unzip` command to extract the csv from the zipped folder."""

!unzip LoanStats_2018Q4.csv.zip

"""We can also use bash/shell commands to look at the raw file using the `!head` and `!tail` commands"""

!head LoanStats_2018Q4.csv

!tail LoanStats_2018Q4.csv

"""As we look at the raw file itself, do you see anything that might cause us trouble as we read in the CSV file to a dataframe?"""

# Read in the CSV

df = pd.read_csv('LoanStats_2018Q4.csv', header=1)

print(df.shape)
df.head()

df.dtypes

df = pd.read_csv('LoanStats_2018Q4.csv', skiprows=1)

print(df.shape)
df.head()

"""The extra rows at the top and bottom of the file have done two things:

1) The top row has made it so that the entire dataset is being interpreted as column headers

2) The bottom two rows have been read into the 'id' column and are causing every column to have at least two `NaN` values in it.

Lets look at the NaN values of each column so that you can see the problem that the extra rows at the bottom of the file are creating for us
"""

# Sum null values by column and sort from least to greatest

df.isnull().sum().sort_values()

df.tail()

# Address the extra NaNs in each column by skipping the footer as well.
df = pd.read_csv('LoanStats_2018Q4.csv', skiprows=1, skipfooter=2)

print(df.shape)
df.tail()

df.isnull().sum().sort_values()

df.dtypes

"""For good measure, we'll also drop some columns that are made up completely of NaN values.

Why might LendingClub have included columns in their dataset that are 100% blank?
"""

df.head()

df = df.drop(['id', 'member_id', 'desc', 'url'], axis=1)

df.head()

"""### Cleaning Up the `int_rate` Column

When we're preparing a dataset for a machine learning model we typically want to represent don't want to leave any string values in our dataset --because it's hard to do math on words. 

Specifically, we have a column that is representing a numeric value, but currently doesn't have a numeric datatype. Lets look at the first 10 values of the `int_rate` column
"""

# Look at the first 10 values of the int_rate column

df['int_rate'].head(10)

# Look at a specific value from the int_rate column

df['int_rate'][0]

df['int_rate'][20]

"""Problems that we need to address with this column:

- String column that should be numeric
- Percent Sign `%` included with the number
- Leading space at the beginning of the string

However, we're not going to try and write exactly the right code to fix this column in one go. We're going to methodically build up to the code that will help us address these problems.

"""

# Lets start with just fixing a single string.
# If we can fix one, we can usually fix all of them

int_rate = ' 14.47%'

my_numb = 14

int_rate.strip()

# my_numb.strip()



# "Chaining"
int_rate.strip().strip('%')

# "Cast" the string value to a float
# "cast" -> Change something's data type
# This is the line of code that we're after! ->
float(int_rate.strip().strip('%'))

type(float(int_rate.strip().strip('%')))

type(int_rate.strip().strip('%'))



"""### Write a function to make our solution reusable!"""

df['int_rate'].head()

# Write a function that can do what we have written above to any 
# string that is passsed to it.
def int_rate_to_float(cell_contents):
  return float(cell_contents.strip().strip('%'))

# Test out our function by calling it on our example
int_rate_to_float(int_rate)

# is the data type correct?
type(int_rate_to_float(int_rate))

"""### `.apply()` our solution to every cell in a column"""

df['int_rate'].apply(int_rate_to_float)

# pass in *only* the name of the function, don't call it. 
# This works because we know the function works on every item in the column
# so I can siply "apply" it to the entire column
df['int_rate_cleaned'] = df['int_rate'].apply(int_rate_to_float)

df.head()

# What type of data is held in our new column?

# Look at the datatypes of the last 5 columns

df.dtypes.tail()

"""### Alternative Approach (not using `.apply()`)"""

clean_int_rates = []

for int_rate in df['int_rate']:
  clean_int_rates.append(int_rate_to_float(int_rate))

clean_int_rates

# Take that list and turn it into a datafram column "Series"

df['cleaned_int_rate2'] = pd.Series(clean_int_rates)

df.head()

df['cleaned_int_rate3'] = int_rate_to_float(' 14.47%')

df.head()

"""### Key Point(s)

We can create a new column with our cleaned values or overwrite the original, whatever we think best suits our needs. On your assignment you will take the same approach in trying to methodically build up the complexity of your code until you have a few lines that will work for any cell in a column. At that point you'll contain all of that functionality in a reusable function block and then use the `.apply()` function to... well... apply those changes to an entire column.

## Modifying/Creating Columns with Pandas

We're already seen one example of using the `.apply()` function to clean up a column. Lets see if we can do it again, but this time on a slightly more complicated use case.

Remember, the goal here is to write a function that will work correctly on any **individual** cell of a specific column. Then we can reuse that function on those individual cells of a dataframe column via the `.apply()` function.

Lets clean up the `emp_title` "Employment Title" column!

First we'll try and diagnose how bad the problem is and what improvements we might be able to make.
"""

# Look at the top 20 employment titles

df['emp_title'].value_counts(dropna=False).head(20)

# How many different unique employment titles are there currently?
df['emp_title'].nunique()

# How often is the employment_title null?
df['emp_title'].isnull().sum()

"""What are some possible reasons as to why a person's employment title may have not been provided?"""

# Create some examples that represent the cases that we want to clean up
examples = ['owner', 'Supervisor', ' Project Manager', 'TEACHER', np.NaN]

# Write a function to clean up these use cases and increase uniformity.
def clean_emp_title(title):
  # type checking
  if isinstance(title, str):
    # Give "title" casing and remove leading/trailing whitespace
    return title.strip().title()
  else:
    return "Unknown"

for example in examples:
  print(clean_emp_title(example))

# The value of the last thing to be returned gets printed out
clean_emp_title('TEACHER')

# if the last thing to be returned gets saved to a variable
# then it won't get printed out

# if I include just a variable name, it will get printed out



df['emp_title_cleaned'] = df['emp_title'].apply(clean_emp_title)

df.head()

# list comprehensions can combine function calls and for loops over lists
# into one succinct and fairly readable single line of code.

# Combining a for loop and adding items to a list into a single line

# We have a function that works as expected. Lets apply it to our column.
# This time we'll overwrite the original column

"""We can use the same code as we did earlier to see how much progress was made.

"""

# Look at the top 20 employment titles

# How many different unique employment titles are there currently?

# How often is the employment_title null (NaN)?'

"""### Key Point(s)

Using the .apply() function isn't always about creating new columns on a dataframe, we can use it to clean up or modify existing columns as well.

## Working with Dates & Times in Pandas

Pandas has its own datatype datatype that makes it extremely convenient to convert strings that are in standard date formates to datetime objects and then use those datetime objects to either create new features on a dataframe or work with the dataset in a timeseries fashion. 

This section will demonstrate how to take a column of date strings, convert it to a datetime object and then use the datetime formatting `.dt` to access specific parts of the date (year, month, day) to generate useful columns on a dataframe.

Pandas documentation:
- [to_datetime](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html)
- [Time/Date Components](https://pandas.pydata.org/pandas-docs/stable/timeseries.html#time-date-components) "You can access these properties via the `.dt` accessor"

Many of the most useful date columns in this dataset have the suffix `_d` to indicate that they correspond to dates.

We'll use a list comprehension to print them out
"""

for col in df:
  if col.endswith('_d'):
    print(col)

"""Lets look at the string format of the `issue_d` column"""

df['issue_d'].head()

df['issue_d'][35]

"""Because this string format %m-%y is a common datetime format, we can just let Pandas detect this format and translate it to the appropriate datetime object."""

pd.to_datetime(df['issue_d'])

df['issue_d_formatted'] = pd.to_datetime(df['issue_d'], infer_datetime_format=True)

df.head()

"""Now we can see that the `issue_d` column has been changed to hold `datetime` objects.

Lets look at one of the cells specifically to see what a datetime object looks like:
"""

df['issue_d_formatted'].head()

df['issue_d_formatted'][19]

"""You can see how the month and year have been indicated by the strings that were contained in the column previously, and that the rest of the values have been inferred."""

df['issue_d_formatted'].dt.year

"""We can use the `.dt` accessor to now grab specific parts of the datetime object. Lets grab just the year from the all of the cells in the `issue_d` column"""

df['issue_d_formatted'].dt.month

"""Now the month."""



"""It's just that easy! Now, instead of printing them out, lets add these year and month values as new columns on our dataframe. Again, you'll have to scroll all the way over to the right in the table to see the new columns."""

df['issue_year'] = df['issue_d_formatted'].dt.year
df['issue_month'] = df['issue_d_formatted'].dt.month

df.head()

"""Because all of these dates come from Q4 of 2018, the `issue_d` column isn't all that interesting. Lets look at the `earliest_cr_line` column, which is also a string, but that could be converted to datetime format.

We're going to create a new column called `days_from_earliest_credit_to_issue`

It's a long column header, but think about how valuable this piece of information could be. This number will essentially indicate the length of a person's credit history and if that is correlated with repayment or other factors could be a valuable predictor!
"""

df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], infer_datetime_format=True)

(df['issue_d_formatted'] - df['earliest_cr_line'])

df['credit_length_days'] = (df['issue_d_formatted'] - df['earliest_cr_line']).dt.days
df['credit_length_years'] = df['credit_length_days'] / 365
df.head()

"""What we're about to do is so cool! Pandas' datetime format is so smart that we can simply use the subtraction operator `-` in order to calculate the amount of time between two dates. 

Think about everything that's going on under the hood in order to give us such straightforward syntax! Handling months of different lengths, leap years, etc. Pandas datetime objects are seriously powerful!
"""

df['credit_length_years'].max()

df['credit_length_days'].max()

"""What's oldest credit history that was involved in Q4 2018? """



"""25,171 days is ~ 68.96 years of credit history!

## Key Point(s)

Pandas' datetime format is so easy to work with that there's really no excuse for not using dates to make features on a dataframe! Get ready to practice more of this on your assignment.

## Some Notebook Tips
"""

# notebook cells will only print out the last thing to be returned in the cell.
df.head()
df.shape

# If I want both to be printed out, I need to "print" everyting above the last function call

print(df.shape)
df.head()